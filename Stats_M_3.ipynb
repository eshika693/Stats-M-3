{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Ques1) What is hypothesis testing in statistics?"
      ],
      "metadata": {
        "id": "Xmct3PtBGbSV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Hypothesis testing in statistics is a method used to make decisions or inferences about a population based on sample data. It involves formulating two competing statements: the null hypothesis (usually representing no effect or status quo) and the alternative hypothesis (indicating the presence of an effect or difference). A statistical test is then performed to determine whether the sample data provides sufficient evidence to reject the null hypothesis in favor of the alternative. This process includes selecting a significance level (commonly 0.05), calculating a test statistic, and comparing it to a critical value or using a p-value to assess the strength of the evidence. Hypothesis testing helps researchers and analysts draw conclusions with a quantifiable level of confidence.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "srjglOqfGpWg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques2) What is the null hypothesis, and how does it differ from the alternative hypothesis?"
      ],
      "metadata": {
        "id": "rvlCaBqtGrz8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The null hypothesis, denoted as H0, s a statement in statistics that assumes there is no effect, no difference, or no relationship between variables in a population—it represents the default or status quo. In contrast, the alternative hypothesis, denoted as H1 or Ha is a statement that contradicts the null hypothesis and suggests that there is a significant effect, difference, or relationship. The goal of hypothesis testing is to use sample data to determine whether there is enough evidence to reject the null hypothesis in favor of the alternative. While the null hypothesis is tested directly and assumed true until evidence suggests otherwise, the alternative hypothesis is what researchers usually hope to support through their analysis."
      ],
      "metadata": {
        "id": "h5NksDBcGvkx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques3)  What is the significance level in hypothesis testing, and why is it important?"
      ],
      "metadata": {
        "id": "0eaVmW-9HBbW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The significance level in hypothesis testing, commonly denoted by 𝛼,s the threshold used to determine whether the null hypothesis should be rejected. It represents the probability of making a Type I error, which occurs when the null hypothesis is wrongly rejected even though it is true. Typical values for α are 0.05, 0.01, or 0.10, with 0.05 being the most commonly used. The significance level is important because it sets the standard for how strong the evidence must be before rejecting the null hypothesis. A lower 𝛼  means stronger evidence is needed to reject H0 reducing the risk of a false positive, but possibly increasing the risk of a Type II error (failing to reject a false null hypothesis). Therefore, choosing an appropriate significance level is crucial for balancing the risks of making incorrect decisions in statistical testing."
      ],
      "metadata": {
        "id": "RnNwh_gMHNKi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques4) What does a P-value represent in hypothesis testing?"
      ],
      "metadata": {
        "id": "ZzsWeDUlHhbT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In hypothesis testing, the p-value represents the probability of obtaining a result as extreme as, or more extreme than, the observed data, assuming that the null hypothesis is true. It measures the strength of evidence against the null hypothesis: a smaller p-value indicates stronger evidence that the null hypothesis may not be true. If the p-value is less than or equal to the chosen significance level (e.g., 0.05), the null hypothesis is rejected in favor of the alternative hypothesis. Conversely, a large p-value suggests that the observed data is consistent with the null hypothesis, and there is not enough evidence to reject it. The p-value helps researchers make objective decisions about the validity of their hypotheses based on sample data."
      ],
      "metadata": {
        "id": "ofH3fmsoHlmV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques5)  How do you interpret the P-value in hypothesis testing?"
      ],
      "metadata": {
        "id": "CI9ccgLeHrZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Interpreting the p-value in hypothesis testing involves comparing it to the chosen significance level (usually denoted as α, such as 0.05). If the p-value is less than or equal to α, it indicates that the observed data is unlikely under the null hypothesis, and thus, the null hypothesis is rejected in favor of the alternative hypothesis. This suggests that the results are statistically significant. On the other hand, if the p-value is greater than α, it means the data is consistent with the null hypothesis, and there is not enough evidence to reject it. In this case, the result is considered not statistically significant. It's important to note that the p-value does not measure the probability that the null hypothesis is true or false; instead, it quantifies how likely the observed data would be if the null hypothesis were true."
      ],
      "metadata": {
        "id": "inbSnBk3HwIu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques6) What are Type 1 and Type 2 errors in hypothesis testing?"
      ],
      "metadata": {
        "id": "jwz-hBuqIAxx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In hypothesis testing, Type I and Type II errors are two kinds of incorrect decisions that can be made when evaluating hypotheses:\n",
        "  \n",
        "    A Type I error occurs when the null hypothesis is true but is incorrectly rejected. This is also known as a \"false positive.\" The probability of making a Type I error is represented by the significance level α (e.g., 0.05), meaning there's a 5% chance of rejecting a true null hypothesis.\n",
        "\n",
        "    A Type II error happens when the null hypothesis is false but is incorrectly accepted (i.e., failing to reject it). This is called a \"false negative.\" The probability of making a Type II error is denoted by 𝛽, and 1-β represents the power of the test, or the probability of correctly rejecting a false null hypothesis."
      ],
      "metadata": {
        "id": "TIn0AAZrIIV4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques7) What is the difference between a one-tailed and a two-tailed test in hypothesis testing?"
      ],
      "metadata": {
        "id": "ucUaOQfVIkhF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The difference between a one-tailed and a two-tailed test in hypothesis testing lies in the direction of the effect being tested. A one-tailed test is used when the research hypothesis specifies a directional effect, meaning the alternative hypothesis states that a parameter is either greater than or less than a certain value (e.g., H1:μ > μ0 or H1:μ < μ0).This test focuses on only one end (tail) of the probability distribution. In contrast, a two-tailed test is used when the alternative hypothesis does not specify a direction but only suggests that the parameter is different from the null value (e.g., H1:μ != μ0).It considers both tails of the distribution, testing for the possibility of deviation in either direction. The choice between the two depends on the research question and determines how the critical region is defined and where the rejection of the null hypothesis occurs."
      ],
      "metadata": {
        "id": "xJr3NYoRIpvU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques8) What is the Z-test, and when is it used in hypothesis testing?"
      ],
      "metadata": {
        "id": "9UR3hkzrJQ_P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The Z-test is a statistical method used in hypothesis testing to determine whether there is a significant difference between a sample mean and a population mean, or between two sample means, when the population variance is known and the sample size is large (usually over 30). It relies on the standard normal distribution (Z-distribution) to calculate the test statistic, which measures how many standard deviations the sample mean is from the population mean under the null hypothesis. The Z-test is commonly applied when data are approximately normally distributed or when the sample size is sufficiently large for the Central Limit Theorem to apply. It helps researchers decide whether to reject the null hypothesis based on how extreme the observed data are compared to what would be expected if the null hypothesis were true.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1QkYYYUDJUzt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques9) How do you calculate the Z-score, and what does it represent in hypothesis testing?"
      ],
      "metadata": {
        "id": "L5fCvGDFJZaC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The Z-score is calculated by subtracting the population mean from the sample mean and then dividing the result by the standard error of the mean. Mathematically, it is expressed as\n",
        "\n",
        "    z = {(Sample mean - population mean)/ (σ/√n)}\n",
        "    σ =population standard deviation,\n",
        "    n = sample size\n",
        "    \n",
        "     In hypothesis testing, the Z-score represents how many standard errors the sample mean is away from the population mean under the assumption that the null hypothesis is true. A higher absolute value of the Z-score indicates that the sample mean is farther from the population mean, providing stronger evidence against the null hypothesis. The Z-score is then compared to critical values from the standard normal distribution to decide whether to reject the null hypothesis.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fsUioSYLJdSZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques10) What is the T-distribution, and when should it be used instead of the normal distribution?"
      ],
      "metadata": {
        "id": "js8zx1jvKZQf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The T-distribution, also known as Student's t-distribution, is a probability distribution used in statistics when estimating population parameters and the sample size is small (typically less than 30) or when the population standard deviation is unknown. Unlike the normal distribution, which is symmetrical and bell-shaped with fixed shape, the t-distribution is also bell-shaped but has heavier tails, meaning it accounts for more variability in smaller samples. As the sample size increases, the t-distribution approaches the normal distribution. It is especially useful in t-tests, where the sample data is used to test hypotheses about means. The t-distribution provides more accurate confidence intervals and test results when the standard deviation must be estimated from the sample rather than known in advance."
      ],
      "metadata": {
        "id": "fABtULlIKpMR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques11) What is the difference between a Z-test and a T-test?"
      ],
      "metadata": {
        "id": "0EtMd0hCKx4e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The main difference between a Z-test and a T-test lies in the conditions under which they are used. A Z-test is typically used when the population standard deviation is known and the sample size is large (usually n>30). It relies on the standard normal distribution. In contrast, a T-test is used when the population standard deviation is unknown and the sample size is small (usually n<= 30). It uses the t-distribution, which has heavier tails to account for the increased uncertainty with smaller samples. While both tests are used to compare means and test hypotheses, the T-test is more flexible and commonly used in practical situations where full population parameters are not known. As sample size increases, the T-test results become very similar to those of the Z-test.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OB8CQ2WQK0vs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques12) What is the T-test, and how is it used in hypothesis testing?"
      ],
      "metadata": {
        "id": "bVtNSJ_wLChz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The T-test is a statistical method used in hypothesis testing to determine whether there is a significant difference between the means of two groups or between a sample mean and a known value, especially when the sample size is small and the population standard deviation is unknown. It is based on the t-distribution, which accounts for greater variability in small samples. The T-test calculates a t-statistic, which measures how far the sample mean deviates from the null hypothesis value in terms of estimated standard errors. This value is then compared to a critical value from the t-distribution table based on the chosen significance level and degrees of freedom. If the t-statistic falls in the critical region, the null hypothesis is rejected. T-tests are commonly used in comparing two sample means (independent or paired) or testing a single sample against a population mean, making them essential tools in analyzing experimental and survey data."
      ],
      "metadata": {
        "id": "90sAeLXPLGhc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques13) What is the relationship between Z-test and T-test in hypothesis testing?"
      ],
      "metadata": {
        "id": "zDSMLhokLPjo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The Z-test and T-test are both statistical tools used in hypothesis testing to determine whether there is a significant difference between sample statistics and population parameters, or between two sample means. They share a similar structure and purpose, as both compare a calculated test statistic to a critical value to decide whether to reject the null hypothesis. The main difference lies in the assumptions and conditions under which they are used. The Z-test is used when the population standard deviation is known and the sample size is large, relying on the standard normal distribution. The T-test, on the other hand, is used when the population standard deviation is unknown and the sample size is small, relying on the t-distribution, which adjusts for the added uncertainty. As the sample size increases, the t-distribution approaches the normal distribution, making the T-test results increasingly similar to those of the Z-test. Thus, both tests are closely related and used interchangeably in large samples, but the T-test is more appropriate when dealing with small samples and unknown population variability."
      ],
      "metadata": {
        "id": "mojgnvQeLTj3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques14) What is a confidence interval, and how is it used to interpret statistical results?"
      ],
      "metadata": {
        "id": "vieDNnzVLcfM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- A confidence interval (CI) is a range of values, derived from sample data, that is used to estimate an unknown population parameter—such as a mean or proportion—with a certain level of confidence. It provides not just a point estimate, but a range within which the true value is likely to fall. For example, a 95% confidence interval means that if the same study were repeated many times, approximately 95% of the calculated intervals would contain the true population value. Confidence intervals are used to interpret statistical results by giving a sense of the precision and reliability of the estimate. A narrower interval indicates more precise estimates, while a wider interval suggests more uncertainty. Importantly, if a confidence interval for a difference in means does not include zero, or for a proportion does not include the null value, it suggests a statistically significant result at the corresponding confidence level."
      ],
      "metadata": {
        "id": "k1Qckl_8Lf-x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques15) What is the margin of error, and how does it affect the confidence interval?"
      ],
      "metadata": {
        "id": "UONVYV_bLnWZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The margin of error is a measure of the uncertainty or potential variation in a sample estimate when inferring about a population parameter. It reflects the maximum expected difference between the true population value and the sample estimate due to sampling variability. The margin of error depends on factors such as the sample size, the variability in the data, and the confidence level chosen. A larger margin of error results in a wider confidence interval, indicating more uncertainty about the estimate, while a smaller margin of error leads to a narrower interval, showing greater precision. In essence, the confidence interval is calculated as the sample estimate ± margin of error, meaning that the margin of error directly affects how broad or narrow the interval will be. Reducing the margin of error, often by increasing the sample size, improves the reliability and usefulness of the confidence interval in making decisions based on data."
      ],
      "metadata": {
        "id": "xaeaahv8LrTV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques16) How is Bayes' Theorem used in statistics, and what is its significance?"
      ],
      "metadata": {
        "id": "lL49g3i1L2H_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Bayes' Theorem is a fundamental concept in statistics that describes how to update the probability of a hypothesis based on new evidence. It connects prior probability (what is initially believed about an event or hypothesis) with the likelihood (the probability of the observed data given the hypothesis) to calculate the posterior probability (the updated probability after considering the new data). Mathematically, it is expressed as:\n",
        "\n"
      ],
      "metadata": {
        "id": "Dpa44HVSL6pY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Screenshot 2025-05-19 162638.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "oURjFqCuMNAT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "where, P(H ∣ E) is the probability of the hypothesis H given the evidence E, P(E ∣ H)  is the probability of observing the evidence if the hypothesis is true, P(H) is the prior probability of the hypothesis, P(E) is the total probability of the evidence. Bayes' Theorem is significant because it allows for a dynamic and rational way to revise beliefs in light of new information, making it essential in fields like medical diagnosis, machine learning, risk analysis, and decision-making under uncertainty."
      ],
      "metadata": {
        "id": "QSg9qCMWMWKs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques17)  What is the Chi-square distribution, and when is it used?"
      ],
      "metadata": {
        "id": "HgcDRVacMz0u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The Chi-square distribution is a statistical distribution that is used primarily for analyzing categorical data and testing hypotheses about the variability or association between variables. It is a positively skewed distribution that becomes more symmetrical as the degrees of freedom increase. The Chi-square distribution is used in various statistical tests, most commonly the Chi-square test of independence, which determines whether there is a significant association between two categorical variables, and the Chi-square goodness-of-fit test, which checks whether observed frequencies match expected frequencies under a certain hypothesis. It is also used in tests for variance and in evaluating the fit of models. The Chi-square distribution is significant because it provides a way to assess whether differences between observed and expected data are due to chance or indicate a statistically meaningful pattern."
      ],
      "metadata": {
        "id": "9LWalxV4Nyf2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques18) What is the Chi-square goodness of fit test, and how is it applied?"
      ],
      "metadata": {
        "id": "EEq4QnqhN5pr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The Chi-square goodness of fit test is a statistical method used to determine whether a set of observed categorical data matches an expected distribution. It helps assess whether the differences between observed frequencies and expected frequencies are due to random chance or indicate a significant deviation from the hypothesized distribution. To apply the test, the observed frequencies for each category are compared to the expected frequencies, which are calculated based on a specified theoretical distribution. The test statistic is calculated using the formula:"
      ],
      "metadata": {
        "id": "1Oim3k4CN-j7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Screenshot 2025-05-19 163546.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "nwKrLL30OH1C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "where 𝑂 represents the observed frequency and E represents the expected frequency for each category. The resulting Chi-square value is then compared to a critical value from the Chi-square distribution table, based on the degrees of freedom and chosen significance level. If the calculated value exceeds the critical value, the null hypothesis—that the data fits the expected distribution—is rejected. This test is commonly used in areas like genetics, marketing, and social sciences to evaluate whether sample data conform to a theoretical model.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OO4NwwakOMI-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques19) What is the F-distribution, and when is it used in hypothesis testing?"
      ],
      "metadata": {
        "id": "x-eJyHA8OUup"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The F-distribution is a probability distribution that arises when comparing variances of two populations. It is a right-skewed distribution used primarily in hypothesis testing to analyze whether two sample variances are significantly different. The F-distribution plays a central role in tests such as ANOVA (Analysis of Variance) and the F-test for equality of variances, where it helps determine if group means or variances differ across multiple samples. The shape of the F-distribution depends on two sets of degrees of freedom—one for the numerator and one for the denominator—reflecting the sample sizes involved. Its significance lies in providing a basis to test complex hypotheses about variance and mean differences, especially when comparing more than two groups or assessing model fit in regression analysis."
      ],
      "metadata": {
        "id": "A-RY5kuJPDIN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques20) What is an ANOVA test, and what are its assumptions?"
      ],
      "metadata": {
        "id": "LBpZF111PIZl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ANOVA (Analysis of Variance) is a statistical test used to compare the means of three or more groups to determine if at least one group mean is significantly different from the others. Instead of performing multiple t-tests, ANOVA assesses all groups simultaneously by analyzing the variation between groups and within groups. If the variation between groups is significantly larger than the variation within groups, it suggests that not all group means are equal. The key assumptions of ANOVA include: (1) independence of observations, meaning the data collected from different groups are independent; (2) normality, which assumes the data in each group are approximately normally distributed; and (3) homogeneity of variances, meaning the variances across groups are roughly equal. Meeting these assumptions ensures that the ANOVA results are valid and reliable for drawing conclusions about group differences."
      ],
      "metadata": {
        "id": "p-o4qbXFPMKR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques21) What are the different types of ANOVA tests?"
      ],
      "metadata": {
        "id": "QHRlJzm9PQ2G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- There are several types of ANOVA tests, each suited for different experimental designs and data structures:\n",
        "    1.One-Way ANOVA: Used to compare the means of three or more independent groups based on one factor or independent variable. For example, comparing test scores across different teaching methods.\n",
        "\n",
        "    2.Two-Way ANOVA: Examines the effect of two independent factors simultaneously on a dependent variable, and also tests for interaction effects between the factors. For example, studying the impact of both diet and exercise on weight loss.\n",
        "\n",
        "    3.Repeated Measures ANOVA: Used when the same subjects are measured multiple times under different conditions or over time. It accounts for the correlation between repeated observations on the same subjects.\n",
        "\n",
        "    4.Mixed-Design ANOVA (Split-Plot ANOVA): Combines both between-subjects and within-subjects factors, useful when one factor involves independent groups and another involves repeated measures."
      ],
      "metadata": {
        "id": "Fsn9M5YKPTqv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques22) What is the F-test, and how does it relate to hypothesis testing?"
      ],
      "metadata": {
        "id": "rTf3VVvgPikd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The F-test is a statistical test used to compare variances and assess whether group means are significantly different by analyzing the ratio of variances. It calculates an F-statistic by dividing the variance between groups by the variance within groups. In hypothesis testing, the F-test helps determine whether observed differences in data are likely due to true effects or just random chance. A high F-value suggests that the between-group variance is large relative to the within-group variance, indicating that at least one group mean differs significantly from the others. The F-test is commonly used in ANOVA to test the overall significance of the model and in tests for equality of variances. The calculated F-statistic is compared to a critical value from the F-distribution based on degrees of freedom; if it exceeds this critical value, the null hypothesis of equal variances or means is rejected.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XnqTOSBIPn_J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PRACTICAL PART-1"
      ],
      "metadata": {
        "id": "9BMy2QF2Pr2N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques1) Write a Python program to generate a random variable and display its value?"
      ],
      "metadata": {
        "id": "2Rpkr-lJPzmu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Generate a random integer between 1 and 100\n",
        "random_variable = random.randint(1, 100)\n",
        "\n",
        "print(\"Random integer value:\", random_variable)\n"
      ],
      "metadata": {
        "id": "UGBJFrAwQMIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques2) Generate a discrete uniform distribution using Python and plot the probability mass function (PMF)?"
      ],
      "metadata": {
        "id": "5R8guV20QRcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import randint\n",
        "\n",
        "# Define the range for the discrete uniform distribution\n",
        "low, high = 1, 10  # integers from 1 to 10 inclusive\n",
        "\n",
        "# Create a discrete uniform distribution object\n",
        "dist = randint(low, high + 1)  # scipy randint is [low, high)\n",
        "\n",
        "# Generate values for the support\n",
        "x = np.arange(low, high + 1)\n",
        "\n",
        "# Calculate PMF values for each x\n",
        "pmf = dist.pmf(x)\n",
        "\n",
        "# Plot the PMF\n",
        "plt.stem(x, pmf, use_line_collection=True)\n",
        "plt.title('Discrete Uniform Distribution PMF')\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Probability')\n",
        "plt.xticks(x)\n",
        "plt.ylim(0, max(pmf) + 0.05)\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "AkZkbfXbQVE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques3) Write a Python function to calculate the probability distribution function (PDF) of a Bernoulli distribution?"
      ],
      "metadata": {
        "id": "Mi-XLoE1Qb8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bernoulli_pmf(k, p):\n",
        "    \"\"\"\n",
        "    Calculate the PMF of a Bernoulli distribution.\n",
        "\n",
        "    Parameters:\n",
        "    k (int): The value (0 or 1) for which to calculate the probability.\n",
        "    p (float): The probability of success (1), where 0 <= p <= 1.\n",
        "\n",
        "    Returns:\n",
        "    float: Probability of observing k.\n",
        "    \"\"\"\n",
        "    if k not in [0, 1]:\n",
        "        return 0\n",
        "    return p**k * (1 - p)**(1 - k)\n",
        "\n",
        "# Example usage:\n",
        "p = 0.7  # Probability of success\n",
        "print(\"P(X=1):\", bernoulli_pmf(1, p))\n",
        "print(\"P(X=0):\", bernoulli_pmf(0, p))\n"
      ],
      "metadata": {
        "id": "KbK7UqJ3Qfxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques4) Write a Python script to simulate a binomial distribution with n=10 and p=0.5, then plot its histogram?"
      ],
      "metadata": {
        "id": "rh5vR85UQmZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Parameters\n",
        "n = 10     # number of trials\n",
        "p = 0.5    # probability of success\n",
        "size = 1000  # number of simulations\n",
        "\n",
        "# Simulate binomial distribution\n",
        "data = np.random.binomial(n, p, size)\n",
        "\n",
        "# Plot histogram\n",
        "plt.hist(data, bins=np.arange(n+2)-0.5, edgecolor='black', density=True)\n",
        "plt.title('Binomial Distribution Histogram (n=10, p=0.5)')\n",
        "plt.xlabel('Number of Successes')\n",
        "plt.ylabel('Probability')\n",
        "plt.xticks(range(n+1))\n",
        "plt.grid(axis='y')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mMIOU_KNQpaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques5)  Create a Poisson distribution and visualize it using Python?"
      ],
      "metadata": {
        "id": "wyIQm76KQtc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import poisson\n",
        "\n",
        "# Parameters\n",
        "lambda_ = 4  # average rate (mean number of events)\n",
        "x = np.arange(0, 15)  # range of values\n",
        "\n",
        "# Calculate PMF\n",
        "pmf = poisson.pmf(x, lambda_)\n",
        "\n",
        "# Plot the PMF\n",
        "plt.stem(x, pmf, use_line_collection=True)\n",
        "plt.title('Poisson Distribution PMF (λ = 4)')\n",
        "plt.xlabel('Number of events')\n",
        "plt.ylabel('Probability')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Lx-CX-qTQwy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques6) Write a Python program to calculate and plot the cumulative distribution function (CDF) of a discrete\n",
        "uniform distribution?"
      ],
      "metadata": {
        "id": "Fy3D5FuEQ0Qa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import randint\n",
        "\n",
        "# Define the range for the discrete uniform distribution\n",
        "low, high = 1, 10  # integers from 1 to 10 inclusive\n",
        "\n",
        "# Create a discrete uniform distribution object\n",
        "dist = randint(low, high + 1)  # scipy randint is [low, high)\n",
        "\n",
        "# Generate values for the support\n",
        "x = np.arange(low, high + 1)\n",
        "\n",
        "# Calculate CDF values for each x\n",
        "cdf = dist.cdf(x)\n",
        "\n",
        "# Plot the CDF\n",
        "plt.step(x, cdf, where='post')\n",
        "plt.title('Discrete Uniform Distribution CDF')\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Cumulative Probability')\n",
        "plt.xticks(x)\n",
        "plt.ylim(0, 1.05)\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rkCXrdPpQ7AQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques7)Generate a continuous uniform distribution using NumPy and visualize it?"
      ],
      "metadata": {
        "id": "mm0diqS1RAww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Parameters for continuous uniform distribution\n",
        "low, high = 0, 1  # range\n",
        "\n",
        "# Generate random samples\n",
        "samples = np.random.uniform(low, high, 1000)\n",
        "\n",
        "# Plot histogram to visualize the distribution\n",
        "plt.hist(samples, bins=30, density=True, edgecolor='black', alpha=0.7)\n",
        "\n",
        "# Plot the theoretical PDF as a horizontal line\n",
        "plt.hlines(1/(high - low), low, high, colors='red', label='Theoretical PDF')\n",
        "\n",
        "plt.title('Continuous Uniform Distribution')\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Density')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FRQY9IA5RGO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques8)  Simulate data from a normal distribution and plot its histogram?"
      ],
      "metadata": {
        "id": "vc2RTHUPRKe4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Parameters for the normal distribution\n",
        "mean = 0\n",
        "std_dev = 1\n",
        "sample_size = 1000\n",
        "\n",
        "# Generate random samples from the normal distribution\n",
        "data = np.random.normal(mean, std_dev, sample_size)\n",
        "\n",
        "# Plot the histogram\n",
        "plt.hist(data, bins=30, density=True, edgecolor='black', alpha=0.7)\n",
        "\n",
        "plt.title('Histogram of Normal Distribution')\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Density')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "epvEnt9UROTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques9)  Write a Python function to calculate Z-scores from a dataset and plot them?"
      ],
      "metadata": {
        "id": "EhVATJyORRwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def calculate_and_plot_z_scores(data):\n",
        "    \"\"\"\n",
        "    Calculate Z-scores for the dataset and plot them.\n",
        "\n",
        "    Parameters:\n",
        "    data (array-like): The input dataset.\n",
        "\n",
        "    Returns:\n",
        "    z_scores (numpy array): Calculated Z-scores.\n",
        "    \"\"\"\n",
        "    data = np.array(data)\n",
        "    mean = np.mean(data)\n",
        "    std_dev = np.std(data, ddof=1)  # sample standard deviation\n",
        "\n",
        "    # Calculate Z-scores\n",
        "    z_scores = (data - mean) / std_dev\n",
        "\n",
        "    # Plot Z-scores\n",
        "    plt.stem(range(len(z_scores)), z_scores, use_line_collection=True)\n",
        "    plt.title('Z-scores of the Dataset')\n",
        "    plt.xlabel('Index')\n",
        "    plt.ylabel('Z-score')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    return z_scores\n",
        "\n",
        "# Example usage:\n",
        "sample_data = [10, 12, 23, 23, 16, 23, 21, 16]\n",
        "z = calculate_and_plot_z_scores(sample_data)\n",
        "print(\"Z-scores:\", z)\n"
      ],
      "metadata": {
        "id": "4ZpNudMXRV6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques10)Implement the Central Limit Theorem (CLT) using Python for a non-normal distribution?"
      ],
      "metadata": {
        "id": "E7_QA0fZRZ50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import norm\n",
        "\n",
        "# Set style for nicer plots\n",
        "sns.set(style='whitegrid')\n",
        "\n",
        "# Parameters\n",
        "population_size = 100000\n",
        "sample_sizes = [5, 30, 100]  # Different sample sizes to illustrate CLT\n",
        "lambda_exp = 1.0  # Parameter for exponential distribution (mean = 1/lambda)\n",
        "num_samples = 1000  # Number of samples to take for each sample size\n",
        "\n",
        "# Generate a large population from an exponential distribution (non-normal)\n",
        "population = np.random.exponential(scale=1/lambda_exp, size=population_size)\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "for i, n in enumerate(sample_sizes, 1):\n",
        "    # Draw 'num_samples' samples of size n and compute their means\n",
        "    sample_means = [np.mean(np.random.choice(population, n, replace=False)) for _ in range(num_samples)]\n",
        "\n",
        "    # Plot histogram of sample means\n",
        "    plt.subplot(1, 3, i)\n",
        "    sns.histplot(sample_means, kde=True, stat=\"density\", bins=30, color='skyblue')\n",
        "\n",
        "    # Overlay a normal distribution curve with same mean and std as sample means\n",
        "    mean_sm = np.mean(sample_means)\n",
        "    std_sm = np.std(sample_means, ddof=1)\n",
        "    x = np.linspace(min(sample_means), max(sample_means), 100)\n",
        "    plt.plot(x, norm.pdf(x, mean_sm, std_sm), 'r--', label='Normal PDF')\n",
        "\n",
        "    plt.title(f'Sample Size = {n}')\n",
        "    plt.xlabel('Sample Mean')\n",
        "    plt.ylabel('Density')\n",
        "    plt.legend()\n",
        "\n",
        "plt.suptitle('Central Limit Theorem Demonstration\\nSampling from Exponential Distribution')\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_SVDb1beRdD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques11) Simulate multiple samples from a normal distribution and verify the Central Limit Theorem?"
      ],
      "metadata": {
        "id": "skdkdrb5RjA5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import norm\n",
        "\n",
        "# Set style for better visuals\n",
        "sns.set(style='whitegrid')\n",
        "\n",
        "# Parameters\n",
        "population_mean = 50\n",
        "population_std = 10\n",
        "sample_size = 30\n",
        "num_samples = 1000\n",
        "\n",
        "# Generate population (large sample) from normal distribution\n",
        "population = np.random.normal(population_mean, population_std, 100000)\n",
        "\n",
        "# Draw many samples of size 'sample_size' and compute their means\n",
        "sample_means = [np.mean(np.random.choice(population, sample_size, replace=False)) for _ in range(num_samples)]\n",
        "\n",
        "# Plot histogram of sample means\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.histplot(sample_means, bins=30, stat=\"density\", kde=True, color='lightblue')\n",
        "\n",
        "# Overlay theoretical normal distribution curve\n",
        "mean_sm = np.mean(sample_means)\n",
        "std_sm = np.std(sample_means, ddof=1)\n",
        "x = np.linspace(min(sample_means), max(sample_means), 100)\n",
        "plt.plot(x, norm.pdf(x, mean_sm, std_sm), 'r--', label='Normal PDF')\n",
        "\n",
        "plt.title('Distribution of Sample Means (Sample size = {})'.format(sample_size))\n",
        "plt.xlabel('Sample Mean')\n",
        "plt.ylabel('Density')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Mean of sample means: {mean_sm:.2f}\")\n",
        "print(f\"Standard deviation of sample means: {std_sm:.2f}\")\n",
        "print(f\"Theoretical std dev of sample means: {population_std/np.sqrt(sample_size):.2f}\")\n"
      ],
      "metadata": {
        "id": "h3kZqJNgRnAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques12) Write a Python function to calculate and plot the standard normal distribution (mean = 0, std = 1)?"
      ],
      "metadata": {
        "id": "2Nml0oQpRsXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "\n",
        "def plot_standard_normal():\n",
        "    # Define the range for x values (typically -4 to 4 covers almost all probability)\n",
        "    x = np.linspace(-4, 4, 1000)\n",
        "\n",
        "    # Calculate the PDF of the standard normal distribution\n",
        "    pdf = norm.pdf(x, loc=0, scale=1)\n",
        "\n",
        "    # Plot the PDF\n",
        "    plt.plot(x, pdf, label='Standard Normal PDF', color='blue')\n",
        "    plt.fill_between(x, pdf, alpha=0.2, color='blue')\n",
        "\n",
        "    plt.title('Standard Normal Distribution (mean=0, std=1)')\n",
        "    plt.xlabel('x')\n",
        "    plt.ylabel('Probability Density')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Call the function\n",
        "plot_standard_normal()\n"
      ],
      "metadata": {
        "id": "xI6WPA7CRvVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques13) Generate random variables and calculate their corresponding probabilities using the binomial distribution?"
      ],
      "metadata": {
        "id": "JVWFQEG3Rz9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import binom\n",
        "\n",
        "# Parameters for the binomial distribution\n",
        "n = 10       # number of trials\n",
        "p = 0.5      # probability of success\n",
        "sample_size = 20  # how many random variables to generate\n",
        "\n",
        "# Generate random variables (number of successes) from the binomial distribution\n",
        "random_vars = np.random.binomial(n, p, sample_size)\n",
        "\n",
        "# Calculate the probability (PMF) of each generated random variable\n",
        "probabilities = binom.pmf(random_vars, n, p)\n",
        "\n",
        "# Display results\n",
        "for rv, prob in zip(random_vars, probabilities):\n",
        "    print(f\"Random variable (successes): {rv}, Probability: {prob:.4f}\")\n"
      ],
      "metadata": {
        "id": "4oYELphwR7Rf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques14) Write a Python program to calculate the Z-score for a given data point and compare it to a standard normal\n",
        "distribution?"
      ],
      "metadata": {
        "id": "_IUyL5F8SOvX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "\n",
        "def calculate_z_score(data, point):\n",
        "    \"\"\"\n",
        "    Calculate the Z-score of a given data point.\n",
        "\n",
        "    Parameters:\n",
        "    data (array-like): Dataset\n",
        "    point (float): Data point to calculate Z-score for\n",
        "\n",
        "    Returns:\n",
        "    float: Z-score of the point\n",
        "    \"\"\"\n",
        "    mean = np.mean(data)\n",
        "    std_dev = np.std(data, ddof=1)  # sample standard deviation\n",
        "    z = (point - mean) / std_dev\n",
        "    return z\n",
        "\n",
        "def plot_z_score_on_normal(z):\n",
        "    \"\"\"\n",
        "    Plot the standard normal distribution and mark the Z-score.\n",
        "\n",
        "    Parameters:\n",
        "    z (float): Z-score to mark on the plot\n",
        "    \"\"\"\n",
        "    x = np.linspace(-4, 4, 1000)\n",
        "    y = norm.pdf(x, 0, 1)\n",
        "\n",
        "    plt.plot(x, y, label='Standard Normal Distribution')\n",
        "    plt.fill_between(x, y, where=(x <= z), color='skyblue', alpha=0.5, label=f'Area ≤ Z={z:.2f}')\n",
        "    plt.axvline(z, color='red', linestyle='--', label=f'Z = {z:.2f}')\n",
        "\n",
        "    plt.title('Z-score on Standard Normal Distribution')\n",
        "    plt.xlabel('Z')\n",
        "    plt.ylabel('Probability Density')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Example usage:\n",
        "data = [12, 15, 14, 10, 18, 20, 22, 17, 19, 16]\n",
        "point = 18\n",
        "\n",
        "z_score = calculate_z_score(data, point)\n",
        "print(f\"Z-score of data point {point}: {z_score:.2f}\")\n",
        "\n",
        "plot_z_score_on_normal(z_score)\n"
      ],
      "metadata": {
        "id": "n-2_5BdWSRx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques15) Implement hypothesis testing using Z-statistics for a sample dataset?"
      ],
      "metadata": {
        "id": "MKHujJItSXBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "\n",
        "def z_test(sample, pop_mean, pop_std, alpha=0.05):\n",
        "    \"\"\"\n",
        "    Performs a two-tailed Z-test for the sample mean.\n",
        "\n",
        "    Parameters:\n",
        "    sample (array-like): Sample data\n",
        "    pop_mean (float): Population mean under null hypothesis\n",
        "    pop_std (float): Population standard deviation (assumed known)\n",
        "    alpha (float): Significance level\n",
        "\n",
        "    Returns:\n",
        "    dict: Z-statistic, p-value, conclusion\n",
        "    \"\"\"\n",
        "    n = len(sample)\n",
        "    sample_mean = np.mean(sample)\n",
        "\n",
        "    # Calculate Z-statistic\n",
        "    z_stat = (sample_mean - pop_mean) / (pop_std / np.sqrt(n))\n",
        "\n",
        "    # Calculate two-tailed p-value\n",
        "    p_value = 2 * (1 - norm.cdf(abs(z_stat)))\n",
        "\n",
        "    # Conclusion\n",
        "    if p_value < alpha:\n",
        "        conclusion = \"Reject the null hypothesis.\"\n",
        "    else:\n",
        "        conclusion = \"Fail to reject the null hypothesis.\"\n",
        "\n",
        "    return {\n",
        "        'Z-statistic': z_stat,\n",
        "        'p-value': p_value,\n",
        "        'Conclusion': conclusion\n",
        "    }\n",
        "\n",
        "# Example usage:\n",
        "population_mean = 100\n",
        "population_std = 15  # known population std deviation\n",
        "sample_data = [102, 98, 105, 95, 100, 99, 101, 97, 103, 96]\n",
        "\n",
        "result = z_test(sample_data, population_mean, population_std)\n",
        "print(f\"Z-statistic: {result['Z-statistic']:.3f}\")\n",
        "print(f\"P-value: {result['p-value']:.4f}\")\n",
        "print(f\"Conclusion: {result['Conclusion']}\")\n"
      ],
      "metadata": {
        "id": "xSG9PyXYSaEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques16) Create a confidence interval for a dataset using Python and interpret the result?"
      ],
      "metadata": {
        "id": "9DErKTckSezO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "def confidence_interval(data, confidence=0.95):\n",
        "    \"\"\"\n",
        "    Calculate the confidence interval for the mean of the data.\n",
        "\n",
        "    Parameters:\n",
        "    data (array-like): The sample data\n",
        "    confidence (float): Confidence level (default 95%)\n",
        "\n",
        "    Returns:\n",
        "    tuple: (lower bound, upper bound) of the confidence interval\n",
        "    \"\"\"\n",
        "    data = np.array(data)\n",
        "    n = len(data)\n",
        "    mean = np.mean(data)\n",
        "    std_err = stats.sem(data)  # Standard error of the mean\n",
        "\n",
        "    # t critical value for two-tailed test\n",
        "    t_crit = stats.t.ppf((1 + confidence) / 2, df=n-1)\n",
        "\n",
        "    margin_of_error = t_crit * std_err\n",
        "\n",
        "    return mean - margin_of_error, mean + margin_of_error\n",
        "\n",
        "# Example dataset\n",
        "sample_data = [12, 15, 14, 10, 18, 20, 22, 17, 19, 16]\n",
        "\n",
        "# Calculate 95% confidence interval\n",
        "lower, upper = confidence_interval(sample_data, confidence=0.95)\n",
        "\n",
        "print(f\"Sample mean: {np.mean(sample_data):.2f}\")\n",
        "print(f\"95% Confidence Interval: ({lower:.2f}, {upper:.2f})\")\n"
      ],
      "metadata": {
        "id": "x2pSwaR4Sidw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques17) Generate data from a normal distribution, then calculate and interpret the confidence interval for its mean?"
      ],
      "metadata": {
        "id": "9RofbPGVSm37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# Step 1: Generate data from a normal distribution\n",
        "np.random.seed(42)  # for reproducibility\n",
        "mean_true = 50\n",
        "std_true = 5\n",
        "sample_size = 100\n",
        "\n",
        "data = np.random.normal(loc=mean_true, scale=std_true, size=sample_size)\n",
        "\n",
        "# Step 2: Calculate the 95% confidence interval for the mean\n",
        "def confidence_interval(data, confidence=0.95):\n",
        "    n = len(data)\n",
        "    mean = np.mean(data)\n",
        "    std_err = stats.sem(data)  # standard error of the mean\n",
        "    t_crit = stats.t.ppf((1 + confidence) / 2, df=n-1)\n",
        "    margin_error = t_crit * std_err\n",
        "    return mean - margin_error, mean + margin_error\n",
        "\n",
        "lower, upper = confidence_interval(data)\n",
        "\n",
        "# Step 3: Print results\n",
        "sample_mean = np.mean(data)\n",
        "print(f\"Sample mean: {sample_mean:.2f}\")\n",
        "print(f\"95% Confidence interval for the mean: ({lower:.2f}, {upper:.2f})\")\n",
        "\n",
        "# Interpretation\n",
        "print(\"\\nInterpretation:\")\n",
        "print(f\"We are 95% confident that the true population mean lies between {lower:.2f} and {upper:.2f}.\")\n"
      ],
      "metadata": {
        "id": "CAbV32ekSrtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques18) Write a Python script to calculate and visualize the probability density function (PDF) of a normal distribution?"
      ],
      "metadata": {
        "id": "WyZ-4d-dSxJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "\n",
        "# Parameters for the normal distribution\n",
        "mean = 0\n",
        "std_dev = 1\n",
        "\n",
        "# Generate x values\n",
        "x = np.linspace(mean - 4*std_dev, mean + 4*std_dev, 1000)\n",
        "\n",
        "# Calculate the PDF values\n",
        "pdf = norm.pdf(x, loc=mean, scale=std_dev)\n",
        "\n",
        "# Plot the PDF\n",
        "plt.plot(x, pdf, label=f'Normal PDF\\nmean={mean}, std={std_dev}', color='blue')\n",
        "plt.fill_between(x, pdf, alpha=0.2, color='blue')\n",
        "plt.title('Probability Density Function of Normal Distribution')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('Density')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "k3x5c5j3S0g6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques19)  Use Python to calculate and interpret the cumulative distribution function (CDF) of a Poisson distribution?"
      ],
      "metadata": {
        "id": "xGuFMBBhS4PG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import poisson\n",
        "\n",
        "# Parameters for the Poisson distribution\n",
        "lambda_param = 3  # average rate (mean)\n",
        "max_k = 15        # max value for k (number of events)\n",
        "\n",
        "# Generate k values (discrete values of the random variable)\n",
        "k_values = np.arange(0, max_k + 1)\n",
        "\n",
        "# Calculate CDF values\n",
        "cdf_values = poisson.cdf(k_values, mu=lambda_param)\n",
        "\n",
        "# Print some example CDF values\n",
        "for k, cdf_val in zip(k_values, cdf_values):\n",
        "    print(f\"P(X ≤ {k}) = {cdf_val:.4f}\")\n",
        "\n",
        "# Plot the CDF\n",
        "plt.step(k_values, cdf_values, where='post', label=f'Poisson CDF (λ={lambda_param})', color='purple')\n",
        "plt.xlabel('Number of events (k)')\n",
        "plt.ylabel('Cumulative Probability P(X ≤ k)')\n",
        "plt.title('CDF of Poisson Distribution')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_IQeqDDsS7ww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques20) Simulate a random variable using a continuous uniform distribution and calculate its expected value?"
      ],
      "metadata": {
        "id": "ii9DQR6pTAc0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the interval [a, b]\n",
        "a = 2\n",
        "b = 10\n",
        "\n",
        "# Simulate a single random variable from Uniform(a, b)\n",
        "random_var = np.random.uniform(a, b)\n",
        "print(f\"Simulated random variable: {random_var:.4f}\")\n",
        "\n",
        "# Simulate many samples to approximate expected value\n",
        "samples = np.random.uniform(a, b, size=100000)\n",
        "simulated_mean = np.mean(samples)\n",
        "print(f\"Simulated expected value (mean) from samples: {simulated_mean:.4f}\")\n",
        "\n",
        "# Theoretical expected value of Uniform(a, b)\n",
        "theoretical_mean = (a + b) / 2\n",
        "print(f\"Theoretical expected value (mean): {theoretical_mean:.4f}\")\n"
      ],
      "metadata": {
        "id": "W_T8ANhmTE5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques21) Write a Python program to compare the standard deviations of two datasets and visualize the difference?"
      ],
      "metadata": {
        "id": "ndKsnFUrTOgX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Example datasets\n",
        "data1 = np.random.normal(loc=50, scale=5, size=1000)  # std dev ~5\n",
        "data2 = np.random.normal(loc=50, scale=10, size=1000) # std dev ~10\n",
        "\n",
        "# Calculate standard deviations\n",
        "std1 = np.std(data1, ddof=1)\n",
        "std2 = np.std(data2, ddof=1)\n",
        "\n",
        "print(f\"Standard deviation of dataset 1: {std1:.2f}\")\n",
        "print(f\"Standard deviation of dataset 2: {std2:.2f}\")\n",
        "\n",
        "# Plot histograms of both datasets\n",
        "plt.figure(figsize=(12,5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(data1, bins=30, alpha=0.7, color='blue', label=f'Data 1 (std={std1:.2f})')\n",
        "plt.hist(data2, bins=30, alpha=0.5, color='orange', label=f'Data 2 (std={std2:.2f})')\n",
        "plt.title('Histogram of Datasets')\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "\n",
        "# Bar plot for standard deviations\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.bar(['Dataset 1', 'Dataset 2'], [std1, std2], color=['blue', 'orange'])\n",
        "plt.title('Comparison of Standard Deviations')\n",
        "plt.ylabel('Standard Deviation')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "NcZhvlBSTRi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques22) Calculate the range and interquartile range (IQR) of a dataset generated from a normal distribution?"
      ],
      "metadata": {
        "id": "iSipqldzTWTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import iqr\n",
        "\n",
        "# Generate a sample dataset from a normal distribution\n",
        "np.random.seed(42)\n",
        "data = np.random.normal(loc=50, scale=10, size=1000)\n",
        "\n",
        "# Calculate range\n",
        "data_range = np.max(data) - np.min(data)\n",
        "\n",
        "# Calculate interquartile range (IQR)\n",
        "data_iqr = iqr(data)\n",
        "\n",
        "print(f\"Range of the dataset: {data_range:.2f}\")\n",
        "print(f\"Interquartile Range (IQR) of the dataset: {data_iqr:.2f}\")\n"
      ],
      "metadata": {
        "id": "PnVGRAlaTZcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques23) Implement Z-score normalization on a dataset and visualize its transformation?"
      ],
      "metadata": {
        "id": "o5NHWQhQTdAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import zscore\n",
        "\n",
        "# Generate sample data (normal distribution)\n",
        "np.random.seed(42)\n",
        "data = np.random.normal(loc=50, scale=10, size=1000)\n",
        "\n",
        "# Perform Z-score normalization\n",
        "data_normalized = zscore(data)\n",
        "\n",
        "# Plot original vs normalized data distributions\n",
        "plt.figure(figsize=(12,5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(data, bins=30, color='skyblue', edgecolor='black')\n",
        "plt.title('Original Data Distribution')\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(data_normalized, bins=30, color='salmon', edgecolor='black')\n",
        "plt.title('Z-score Normalized Data Distribution')\n",
        "plt.xlabel('Z-score')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Pudayda1TgBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques24) Write a Python function to calculate the skewness and kurtosis of a dataset generated from a normal\n",
        "distribution?"
      ],
      "metadata": {
        "id": "ZLzdtClvVNxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import skew, kurtosis\n",
        "\n",
        "def calculate_skewness_kurtosis(data):\n",
        "    \"\"\"\n",
        "    Calculate skewness and kurtosis of the dataset.\n",
        "\n",
        "    Parameters:\n",
        "    data (array-like): Input dataset\n",
        "\n",
        "    Returns:\n",
        "    tuple: (skewness, kurtosis)\n",
        "    \"\"\"\n",
        "    data_skewness = skew(data)\n",
        "    data_kurtosis = kurtosis(data)  # by default, Fisher’s definition (excess kurtosis)\n",
        "    return data_skewness, data_kurtosis\n",
        "\n",
        "# Generate sample data from normal distribution\n",
        "np.random.seed(42)\n",
        "sample_data = np.random.normal(loc=0, scale=1, size=1000)\n",
        "\n",
        "# Calculate skewness and kurtosis\n",
        "skewness, kurt = calculate_skewness_kurtosis(sample_data)\n",
        "\n",
        "print(f\"Skewness: {skewness:.4f}\")\n",
        "print(f\"Kurtosis (excess): {kurt:.4f}\")\n"
      ],
      "metadata": {
        "id": "uvK9aqbGVU3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PRACTICAL  PART 2"
      ],
      "metadata": {
        "id": "hlJuUNcI4kMF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques1) Write a Python program to perform a Z-test for comparing a sample mean to a known population mean and\n",
        "interpret the results?"
      ],
      "metadata": {
        "id": "NfZQxECO4s8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "\n",
        "def z_test(sample_data, population_mean, population_std, alpha=0.05):\n",
        "    # Sample statistics\n",
        "    sample_mean = np.mean(sample_data)\n",
        "    sample_size = len(sample_data)\n",
        "\n",
        "    # Calculate the Z-score\n",
        "    z_score = (sample_mean - population_mean) / (population_std / np.sqrt(sample_size))\n",
        "\n",
        "    # Calculate the p-value for a two-tailed test\n",
        "    p_value = 2 * (1 - norm.cdf(abs(z_score)))\n",
        "\n",
        "    # Interpretation\n",
        "    print(f\"Sample Mean     = {sample_mean:.3f}\")\n",
        "    print(f\"Z-score         = {z_score:.3f}\")\n",
        "    print(f\"P-value         = {p_value:.4f}\")\n",
        "\n",
        "    if p_value < alpha:\n",
        "        print(\"Result: Reject the null hypothesis (significant difference).\")\n",
        "    else:\n",
        "        print(\"Result: Fail to reject the null hypothesis (no significant difference).\")\n",
        "\n",
        "# Example usage\n",
        "sample = [105, 110, 98, 100, 102, 108, 107, 99, 101, 103]  # Sample data\n",
        "population_mean = 100\n",
        "population_std = 5\n",
        "\n",
        "z_test(sample, population_mean, population_std)\n"
      ],
      "metadata": {
        "id": "4vL_is6g4uxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques2)  Simulate random data to perform hypothesis testing and calculate the corresponding P-value using Python?"
      ],
      "metadata": {
        "id": "guu35p3U5SEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "\n",
        "# Step 1: Simulate random data\n",
        "np.random.seed(42)  # For reproducibility\n",
        "population_mean = 100\n",
        "population_std = 15\n",
        "sample_size = 50\n",
        "\n",
        "# Simulate a sample from the population\n",
        "sample_data = np.random.normal(loc=102, scale=15, size=sample_size)  # shift mean to simulate difference\n",
        "\n",
        "# Step 2: Compute sample statistics\n",
        "sample_mean = np.mean(sample_data)\n",
        "\n",
        "# Step 3: Calculate Z-score\n",
        "z_score = (sample_mean - population_mean) / (population_std / np.sqrt(sample_size))\n",
        "\n",
        "# Step 4: Calculate two-tailed P-value\n",
        "p_value = 2 * (1 - norm.cdf(abs(z_score)))\n",
        "\n",
        "# Step 5: Output results\n",
        "print(\"Simulated Hypothesis Test Results\")\n",
        "print(\"---------------------------------\")\n",
        "print(f\"Sample Mean         = {sample_mean:.2f}\")\n",
        "print(f\"Population Mean     = {population_mean}\")\n",
        "print(f\"Z-score             = {z_score:.3f}\")\n",
        "print(f\"P-value             = {p_value:.4f}\")\n",
        "\n",
        "# Step 6: Conclusion\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Conclusion: Reject the null hypothesis (significant difference).\")\n",
        "else:\n",
        "    print(\"Conclusion: Fail to reject the null hypothesis (no significant difference).\")\n"
      ],
      "metadata": {
        "id": "BkazfZyi5X4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques3) Implement a one-sample Z-test using Python to compare the sample mean with the population mean?"
      ],
      "metadata": {
        "id": "VwPZocW25h6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "\n",
        "def one_sample_z_test(sample, population_mean, population_std, alpha=0.05, alternative='two-sided'):\n",
        "    \"\"\"\n",
        "    Perform a one-sample Z-test.\n",
        "\n",
        "    Parameters:\n",
        "    - sample: list or array of sample observations\n",
        "    - population_mean: known population mean\n",
        "    - population_std: known population standard deviation\n",
        "    - alpha: significance level (default is 0.05)\n",
        "    - alternative: 'two-sided', 'greater', or 'less'\n",
        "\n",
        "    Returns:\n",
        "    - z_score: calculated Z-score\n",
        "    - p_value: calculated P-value\n",
        "    - conclusion: test conclusion string\n",
        "    \"\"\"\n",
        "    sample = np.array(sample)\n",
        "    sample_mean = np.mean(sample)\n",
        "    sample_size = len(sample)\n",
        "\n",
        "    # Calculate Z-score\n",
        "    z_score = (sample_mean - population_mean) / (population_std / np.sqrt(sample_size))\n",
        "\n",
        "    # Calculate P-value based on the alternative hypothesis\n",
        "    if alternative == 'two-sided':\n",
        "        p_value = 2 * (1 - norm.cdf(abs(z_score)))\n",
        "    elif alternative == 'greater':\n",
        "        p_value = 1 - norm.cdf(z_score)\n",
        "    elif alternative == 'less':\n",
        "        p_value = norm.cdf(z_score)\n",
        "    else:\n",
        "        raise ValueError(\"alternative must be 'two-sided', 'greater', or 'less'\")\n",
        "\n",
        "    # Interpretation\n",
        "    if p_value < alpha:\n",
        "        conclusion = \"Reject the null hypothesis\"\n",
        "    else:\n",
        "        conclusion = \"Fail to reject the null hypothesis\"\n",
        "\n",
        "    # Output\n",
        "    print(\"One-Sample Z-Test Results\")\n",
        "    print(\"-------------------------\")\n",
        "    print(f\"Sample Mean         = {sample_mean:.2f}\")\n",
        "    print(f\"Z-score             = {z_score:.3f}\")\n",
        "    print(f\"P-value             = {p_value:.4f}\")\n",
        "    print(f\"Conclusion          = {conclusion}\")\n",
        "\n",
        "    return z_score, p_value, conclusion\n"
      ],
      "metadata": {
        "id": "KSLfRL0h5l4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques4) Perform a two-tailed Z-test using Python and visualize the decision region on a plot?"
      ],
      "metadata": {
        "id": "VqACdv5b52vP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "\n",
        "def two_tailed_z_test(sample, population_mean, population_std, alpha=0.05):\n",
        "    # Convert to numpy array\n",
        "    sample = np.array(sample)\n",
        "    sample_size = len(sample)\n",
        "    sample_mean = np.mean(sample)\n",
        "\n",
        "    # Calculate Z-score\n",
        "    z_score = (sample_mean - population_mean) / (population_std / np.sqrt(sample_size))\n",
        "\n",
        "    # Calculate two-tailed P-value\n",
        "    p_value = 2 * (1 - norm.cdf(abs(z_score)))\n",
        "\n",
        "    # Determine critical Z-values\n",
        "    z_critical = norm.ppf(1 - alpha/2)\n",
        "\n",
        "    # Output results\n",
        "    print(\"Two-Tailed Z-Test\")\n",
        "    print(\"------------------\")\n",
        "    print(f\"Sample Mean     = {sample_mean:.2f}\")\n",
        "    print(f\"Z-score         = {z_score:.3f}\")\n",
        "    print(f\"P-value         = {p_value:.4f}\")\n",
        "    print(f\"Critical Z      = ±{z_critical:.3f}\")\n",
        "\n",
        "    if abs(z_score) > z_critical:\n",
        "        print(\"Conclusion      = Reject the null hypothesis\")\n",
        "    else:\n",
        "        print(\"Conclusion      = Fail to reject the null hypothesis\")\n",
        "\n",
        "    # Visualization\n",
        "    x = np.linspace(-4, 4, 1000)\n",
        "    y = norm.pdf(x)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(x, y, label=\"Standard Normal Distribution\", color=\"black\")\n",
        "\n",
        "    # Shade critical regions\n",
        "    plt.fill_between(x, y, where=(x <= -z_critical), color='red', alpha=0.5, label=\"Rejection Region\")\n",
        "    plt.fill_between(x, y, where=(x >= z_critical), color='red', alpha=0.5)\n",
        "\n",
        "    # Mark z-score\n",
        "    plt.axvline(z_score, color='blue', linestyle='--', linewidth=2, label=f\"Z = {z_score:.2f}\")\n",
        "\n",
        "    # Labels and legend\n",
        "    plt.title(\"Two-Tailed Z-Test and Decision Regions\")\n",
        "    plt.xlabel(\"Z-score\")\n",
        "    plt.ylabel(\"Probability Density\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    return z_score, p_value\n",
        "\n",
        "# Example usage\n",
        "np.random.seed(42)\n",
        "sample = np.random.normal(loc=102, scale=15, size=50)\n",
        "population_mean = 100\n",
        "population_std = 15\n",
        "\n",
        "two_tailed_z_test(sample, population_mean, population_std)\n"
      ],
      "metadata": {
        "id": "4Ocamg8o59nr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques5) Create a Python function that calculates and visualizes Type 1 and Type 2 errors during hypothesis testing?"
      ],
      "metadata": {
        "id": "NXlotz1Q6TIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "\n",
        "def visualize_type1_type2(pop_mean_null, pop_mean_alt, pop_std, n, alpha=0.05, tail='right'):\n",
        "    \"\"\"\n",
        "    Visualizes Type I and Type II errors for a one-tailed Z-test.\n",
        "\n",
        "    Parameters:\n",
        "    - pop_mean_null: Mean under the null hypothesis (H0)\n",
        "    - pop_mean_alt: Mean under the alternative hypothesis (H1)\n",
        "    - pop_std: Population standard deviation\n",
        "    - n: Sample size\n",
        "    - alpha: Significance level\n",
        "    - tail: 'right' or 'left' for one-tailed test\n",
        "    \"\"\"\n",
        "    se = pop_std / np.sqrt(n)\n",
        "\n",
        "    # Define Z critical value and decision threshold\n",
        "    if tail == 'right':\n",
        "        z_crit = norm.ppf(1 - alpha)\n",
        "        threshold = pop_mean_null + z_crit * se\n",
        "    elif tail == 'left':\n",
        "        z_crit = norm.ppf(alpha)\n",
        "        threshold = pop_mean_null + z_crit * se\n",
        "    else:\n",
        "        raise ValueError(\"tail must be 'right' or 'left'\")\n",
        "\n",
        "    # X values for plotting\n",
        "    x = np.linspace(pop_mean_null - 4*se, pop_mean_alt + 4*se, 1000)\n",
        "\n",
        "    # Null and alternative distributions\n",
        "    null_dist = norm(loc=pop_mean_null, scale=se)\n",
        "    alt_dist = norm(loc=pop_mean_alt, scale=se)\n",
        "\n",
        "    # Plot distributions\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(x, null_dist.pdf(x), label='H₀: Null Distribution', color='blue')\n",
        "    plt.plot(x, alt_dist.pdf(x), label='H₁: Alternative Distribution', color='green')\n",
        "\n",
        "    # Shade Type I Error (α)\n",
        "    if tail == 'right':\n",
        "        plt.fill_between(x, 0, null_dist.pdf(x), where=(x >= threshold), color='red', alpha=0.3, label='Type I Error (α)')\n",
        "    else:\n",
        "        plt.fill_between(x, 0, null_dist.pdf(x), where=(x <= threshold), color='red', alpha=0.3, label='Type I Error (α)')\n",
        "\n",
        "    # Shade Type II Error (β)\n",
        "    if tail == 'right':\n",
        "        plt.fill_between(x, 0, alt_dist.pdf(x), where=(x < threshold), color='orange', alpha=0.3, label='Type II Error (β)')\n",
        "    else:\n",
        "        plt.fill_between(x, 0, alt_dist.pdf(x), where=(x > threshold), color='orange', alpha=0.3, label='Type II Error (β)')\n",
        "\n",
        "    # Add threshold line\n",
        "    plt.axvline(threshold, color='black', linestyle='--', label=f'Decision Threshold = {threshold:.2f}')\n",
        "\n",
        "    # Labels and legend\n",
        "    plt.title(\"Type I and Type II Errors in Hypothesis Testing\")\n",
        "    plt.xlabel(\"Sample Mean\")\n",
        "    plt.ylabel(\"Probability Density\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # Calculate β (Type II error)\n",
        "    if tail == 'right':\n",
        "        beta = alt_dist.cdf(threshold)\n",
        "    else:\n",
        "        beta = 1 - alt_dist.cdf(threshold)\n",
        "\n",
        "    power = 1 - beta\n",
        "\n",
        "    print(f\"Critical Threshold = {threshold:.2f}\")\n",
        "    print(f\"Type I Error (α)   = {alpha:.3f}\")\n",
        "    print(f\"Type II Error (β)  = {beta:.3f}\")\n",
        "    print(f\"Power of Test      = {power:.3f}\")\n"
      ],
      "metadata": {
        "id": "AOyt0TvF6Wsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques6) Write a Python program to perform an independent T-test and interpret the results?"
      ],
      "metadata": {
        "id": "i_3aslA26nXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "def independent_t_test(sample1, sample2, alpha=0.05, equal_var=False):\n",
        "    \"\"\"\n",
        "    Perform an independent t-test on two samples.\n",
        "\n",
        "    Parameters:\n",
        "    - sample1, sample2: Lists or arrays of numeric data\n",
        "    - alpha: Significance level (default 0.05)\n",
        "    - equal_var: If True, assume equal population variances (standard t-test).\n",
        "                 If False, Welch's t-test (default and safer).\n",
        "    \"\"\"\n",
        "    # Convert to NumPy arrays\n",
        "    sample1 = np.array(sample1)\n",
        "    sample2 = np.array(sample2)\n",
        "\n",
        "    # Perform the t-test\n",
        "    t_stat, p_value = ttest_ind(sample1, sample2, equal_var=equal_var)\n",
        "\n",
        "    # Print sample stats\n",
        "    print(\"Group 1 Mean =\", round(np.mean(sample1), 2))\n",
        "    print(\"Group 2 Mean =\", round(np.mean(sample2), 2))\n",
        "    print(\"T-statistic  =\", round(t_stat, 3))\n",
        "    print(\"P-value      =\", round(p_value, 4))\n",
        "\n",
        "    # Interpretation\n",
        "    if p_value < alpha:\n",
        "        print(\"Conclusion: Reject the null hypothesis (means are significantly different).\")\n",
        "    else:\n",
        "        print(\"Conclusion: Fail to reject the null hypothesis (no significant difference).\")\n",
        "\n",
        "# Example usage\n",
        "np.random.seed(1)\n",
        "group1 = np.random.normal(loc=100, scale=10, size=30)\n",
        "group2 = np.random.normal(loc=105, scale=12, size=30)\n",
        "\n",
        "independent_t_test(group1, group2, alpha=0.05, equal_var=False)  # Welch's t-test\n"
      ],
      "metadata": {
        "id": "DfdVssSs6rrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques7)  Perform a paired sample T-test using Python and visualize the comparison results?"
      ],
      "metadata": {
        "id": "7Cno4tNl651X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import ttest_rel\n",
        "\n",
        "def paired_t_test_visual(before, after, alpha=0.05):\n",
        "    \"\"\"\n",
        "    Performs a paired t-test and visualizes the results.\n",
        "\n",
        "    Parameters:\n",
        "    - before, after: Arrays or lists of paired observations (same subjects)\n",
        "    - alpha: Significance level (default 0.05)\n",
        "    \"\"\"\n",
        "    # Convert to NumPy arrays\n",
        "    before = np.array(before)\n",
        "    after = np.array(after)\n",
        "\n",
        "    # Compute paired t-test\n",
        "    t_stat, p_value = ttest_rel(before, after)\n",
        "\n",
        "    # Differences\n",
        "    differences = after - before\n",
        "    mean_diff = np.mean(differences)\n",
        "\n",
        "    # Print results\n",
        "    print(\"Paired T-Test Results\")\n",
        "    print(\"----------------------\")\n",
        "    print(f\"Mean Before      = {np.mean(before):.2f}\")\n",
        "    print(f\"Mean After       = {np.mean(after):.2f}\")\n",
        "    print(f\"Mean Difference  = {mean_diff:.2f}\")\n",
        "    print(f\"T-statistic      = {t_stat:.3f}\")\n",
        "    print(f\"P-value          = {p_value:.4f}\")\n",
        "\n",
        "    if p_value < alpha:\n",
        "        print(\"Conclusion: Reject the null hypothesis (significant difference).\")\n",
        "    else:\n",
        "        print(\"Conclusion: Fail to reject the null hypothesis (no significant difference).\")\n",
        "\n",
        "    # Visualization\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    x = np.arange(len(before))\n",
        "    plt.plot(x, before, marker='o', label='Before', color='blue')\n",
        "    plt.plot(x, after, marker='o', label='After', color='green')\n",
        "    for i in range(len(before)):\n",
        "        plt.plot([x[i], x[i]], [before[i], after[i]], color='gray', linestyle='--', alpha=0.6)\n",
        "\n",
        "    plt.title(\"Paired Sample Comparison (Before vs. After)\")\n",
        "    plt.xlabel(\"Subject\")\n",
        "    plt.ylabel(\"Measurement\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "np.random.seed(42)\n",
        "before = np.random.normal(loc=100, scale=10, size=30)\n",
        "after = before + np.random.normal(loc=2, scale=5, size=30)  # Add a small effect\n",
        "\n",
        "paired_t_test_visual(before, after)\n"
      ],
      "metadata": {
        "id": "ebBU5dzp7D9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques8) Simulate data and perform both Z-test and T-test, then compare the results using Python?"
      ],
      "metadata": {
        "id": "a9uGRU2t7UOX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import ttest_ind, norm\n",
        "\n",
        "def simulate_and_compare_tests(mu1=100, mu2=105, sigma=15, n1=30, n2=30, alpha=0.05):\n",
        "    # Simulate data\n",
        "    np.random.seed(0)\n",
        "    sample1 = np.random.normal(loc=mu1, scale=sigma, size=n1)\n",
        "    sample2 = np.random.normal(loc=mu2, scale=sigma, size=n2)\n",
        "\n",
        "    # ------------------------\n",
        "    # Z-TEST (known population std)\n",
        "    # ------------------------\n",
        "    pooled_std = sigma  # Assuming same known std for both populations\n",
        "    se = np.sqrt(pooled_std**2 / n1 + pooled_std**2 / n2)\n",
        "    z_score = (np.mean(sample1) - np.mean(sample2)) / se\n",
        "    p_value_z = 2 * (1 - norm.cdf(abs(z_score)))\n",
        "\n",
        "    print(\"\\n🔍 Z-Test (Assuming Known Population Std)\")\n",
        "    print(\"-----------------------------------------\")\n",
        "    print(f\"Z-score       = {z_score:.3f}\")\n",
        "    print(f\"P-value       = {p_value_z:.4f}\")\n",
        "    if p_value_z < alpha:\n",
        "        print(\"Conclusion    = Reject the null hypothesis (Z-test)\")\n",
        "    else:\n",
        "        print(\"Conclusion    = Fail to reject the null hypothesis (Z-test)\")\n",
        "\n",
        "    # ------------------------\n",
        "    # T-TEST (unknown population std)\n",
        "    # ------------------------\n",
        "    t_stat, p_value_t = ttest_ind(sample1, sample2, equal_var=True)\n",
        "\n",
        "    print(\"\\n📊 T-Test (Assuming Unknown Population Std)\")\n",
        "    print(\"--------------------------------------------\")\n",
        "    print(f\"T-statistic   = {t_stat:.3f}\")\n",
        "    print(f\"P-value       = {p_value_t:.4f}\")\n",
        "    if p_value_t < alpha:\n",
        "        print(\"Conclusion    = Reject the null hypothesis (T-test)\")\n",
        "    else:\n",
        "        print(\"Conclusion    = Fail to reject the null hypothesis (T-test)\")\n",
        "\n",
        "    # ------------------------\n",
        "    # Summary Comparison\n",
        "    # ------------------------\n",
        "    print(\"\\n📌 Summary\")\n",
        "    print(\"---------\")\n",
        "    print(f\"Sample 1 Mean = {np.mean(sample1):.2f}\")\n",
        "    print(f\"Sample 2 Mean = {np.mean(sample2):.2f}\")\n",
        "    print(f\"Mean Diff     = {np.mean(sample1) - np.mean(sample2):.2f}\")\n",
        "\n",
        "# Run the simulation and comparison\n",
        "simulate_and_compare_tests()\n"
      ],
      "metadata": {
        "id": "tMGpVLO27YKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques9) Write a Python function to calculate the confidence interval for a sample mean and explain its significance?"
      ],
      "metadata": {
        "id": "o1kzfsI_7o_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import norm, t\n",
        "\n",
        "def confidence_interval(data, confidence=0.95, population_std=None):\n",
        "    \"\"\"\n",
        "    Calculate the confidence interval for the sample mean.\n",
        "\n",
        "    Parameters:\n",
        "    - data: List or NumPy array of sample data\n",
        "    - confidence: Confidence level (e.g., 0.95 for 95%)\n",
        "    - population_std: If known, use Z-distribution; if None, use t-distribution\n",
        "\n",
        "    Returns:\n",
        "    - (lower_bound, upper_bound): Tuple representing the confidence interval\n",
        "    \"\"\"\n",
        "    data = np.array(data)\n",
        "    n = len(data)\n",
        "    sample_mean = np.mean(data)\n",
        "    sample_std = np.std(data, ddof=1)\n",
        "\n",
        "    if population_std is not None:\n",
        "        # Use Z-distribution\n",
        "        z_score = norm.ppf(1 - (1 - confidence) / 2)\n",
        "        se = population_std / np.sqrt(n)\n",
        "        margin = z_score * se\n",
        "    else:\n",
        "        # Use t-distribution\n",
        "        df = n - 1\n",
        "        t_score = t.ppf(1 - (1 - confidence) / 2, df)\n",
        "        se = sample_std / np.sqrt(n)\n",
        "        margin = t_score * se\n",
        "\n",
        "    lower_bound = sample_mean - margin\n",
        "    upper_bound = sample_mean + margin\n",
        "\n",
        "    print(f\"Sample Mean      = {sample_mean:.2f}\")\n",
        "    print(f\"Confidence Level = {confidence*100:.1f}%\")\n",
        "    print(f\"Margin of Error  = {margin:.2f}\")\n",
        "    print(f\"Confidence Interval = ({lower_bound:.2f}, {upper_bound:.2f})\")\n",
        "\n",
        "    return lower_bound, upper_bound\n"
      ],
      "metadata": {
        "id": "BDQlvkUX7sXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques10) Write a Python program to calculate the margin of error for a given confidence level using sample data?"
      ],
      "metadata": {
        "id": "ZDy9ULFJ75Wv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import norm, t\n",
        "\n",
        "def calculate_margin_of_error(data, confidence=0.95, population_std=None):\n",
        "    \"\"\"\n",
        "    Calculate the margin of error for a sample mean.\n",
        "\n",
        "    Parameters:\n",
        "    - data: List or NumPy array of sample data\n",
        "    - confidence: Confidence level (default 0.95 for 95%)\n",
        "    - population_std: Known population standard deviation (optional)\n",
        "\n",
        "    Returns:\n",
        "    - Margin of Error (float)\n",
        "    \"\"\"\n",
        "    data = np.array(data)\n",
        "    n = len(data)\n",
        "\n",
        "    if n < 2:\n",
        "        raise ValueError(\"Sample size must be at least 2 to calculate margin of error.\")\n",
        "\n",
        "    if population_std is not None:\n",
        "        # Use Z-distribution\n",
        "        z_score = norm.ppf(1 - (1 - confidence) / 2)\n",
        "        se = population_std / np.sqrt(n)\n",
        "        moe = z_score * se\n",
        "        print(f\"Using Z-distribution with known population std = {population_std}\")\n",
        "    else:\n",
        "        # Use T-distribution\n",
        "        sample_std = np.std(data, ddof=1)\n",
        "        df = n - 1\n",
        "        t_score = t.ppf(1 - (1 - confidence) / 2, df)\n",
        "        se = sample_std / np.sqrt(n)\n",
        "        moe = t_score * se\n",
        "        print(f\"Using T-distribution with sample std = {sample_std:.2f}\")\n",
        "\n",
        "    print(f\"Sample size      = {n}\")\n",
        "    print(f\"Confidence level = {confidence*100:.1f}%\")\n",
        "    print(f\"Margin of Error  = {moe:.3f}\")\n",
        "    return moe\n"
      ],
      "metadata": {
        "id": "TeuIkVkl782h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques11) Implement a Bayesian inference method using Bayes' Theorem in Python and explain the process?"
      ],
      "metadata": {
        "id": "mH4fpIva8IFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bayes_theorem(prior_h, likelihood_h, prior_not_h, likelihood_not_h):\n",
        "    \"\"\"\n",
        "    Calculate the posterior probability P(H|D) using Bayes' Theorem.\n",
        "\n",
        "    Parameters:\n",
        "    - prior_h: P(H), prior probability of hypothesis\n",
        "    - likelihood_h: P(D|H), likelihood of data under hypothesis\n",
        "    - prior_not_h: P(~H), prior probability of negation\n",
        "    - likelihood_not_h: P(D|~H), likelihood of data under negation\n",
        "\n",
        "    Returns:\n",
        "    - posterior probability P(H|D)\n",
        "    \"\"\"\n",
        "    # Calculate total probability of data P(D)\n",
        "    p_data = likelihood_h * prior_h + likelihood_not_h * prior_not_h\n",
        "\n",
        "    # Apply Bayes' theorem\n",
        "    posterior = (likelihood_h * prior_h) / p_data\n",
        "\n",
        "    return posterior\n",
        "\n",
        "# Parameters for the disease testing example\n",
        "P_disease = 0.01           # Prior probability of disease\n",
        "P_no_disease = 1 - P_disease\n",
        "\n",
        "sensitivity = 0.99         # P(Positive test | Disease)\n",
        "false_positive_rate = 0.05 # P(Positive test | No disease)\n",
        "\n",
        "# Calculate posterior probability of having disease given positive test\n",
        "posterior_prob = bayes_theorem(P_disease, sensitivity, P_no_disease, false_positive_rate)\n",
        "\n",
        "print(f\"Probability of having the disease given a positive test: {posterior_prob:.4f}\")\n"
      ],
      "metadata": {
        "id": "OdRyx25C8MtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques12) E Perform a Chi-square test for independence between two categorical variables in Python?"
      ],
      "metadata": {
        "id": "tW_v_iZV8RDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Example categorical data (can be replaced with your own)\n",
        "data = {\n",
        "    'Gender': ['Male', 'Male', 'Female', 'Female', 'Male', 'Female', 'Male', 'Female', 'Female', 'Male'],\n",
        "    'Preference': ['A', 'B', 'A', 'B', 'A', 'A', 'B', 'B', 'A', 'B']\n",
        "}\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Create a contingency table\n",
        "contingency_table = pd.crosstab(df['Gender'], df['Preference'])\n",
        "\n",
        "print(\"Contingency Table:\")\n",
        "print(contingency_table)\n",
        "\n",
        "# Perform the Chi-square test\n",
        "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
        "\n",
        "print(f\"\\nChi-square Statistic = {chi2:.3f}\")\n",
        "print(f\"Degrees of Freedom = {dof}\")\n",
        "print(f\"P-value = {p:.4f}\")\n",
        "print(\"\\nExpected Frequencies:\")\n",
        "print(pd.DataFrame(expected, index=contingency_table.index, columns=contingency_table.columns))\n",
        "\n",
        "# Interpretation\n",
        "alpha = 0.05\n",
        "if p < alpha:\n",
        "    print(\"\\nConclusion: Reject the null hypothesis. Variables are dependent.\")\n",
        "else:\n",
        "    print(\"\\nConclusion: Fail to reject the null hypothesis. Variables are independent.\")\n"
      ],
      "metadata": {
        "id": "njv7OZoO8VhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques13) Write a Python program to calculate the expected frequencies for a Chi-square test based on observed\n",
        "data?"
      ],
      "metadata": {
        "id": "0TdeawiK8ZQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def calculate_expected_frequencies(observed):\n",
        "    \"\"\"\n",
        "    Calculate expected frequencies for Chi-square test given observed data.\n",
        "\n",
        "    Parameters:\n",
        "    - observed: 2D array-like or pandas DataFrame of observed frequencies\n",
        "\n",
        "    Returns:\n",
        "    - expected: pandas DataFrame of expected frequencies with same shape/index/columns\n",
        "    \"\"\"\n",
        "    if not isinstance(observed, pd.DataFrame):\n",
        "        observed = pd.DataFrame(observed)\n",
        "\n",
        "    row_totals = observed.sum(axis=1)\n",
        "    col_totals = observed.sum(axis=0)\n",
        "    grand_total = observed.values.sum()\n",
        "\n",
        "    expected = pd.DataFrame(\n",
        "        np.outer(row_totals, col_totals) / grand_total,\n",
        "        index=observed.index,\n",
        "        columns=observed.columns\n",
        "    )\n",
        "\n",
        "    return expected\n",
        "\n",
        "# Example observed data (contingency table)\n",
        "observed_data = pd.DataFrame({\n",
        "    'Category A': [10, 20, 30],\n",
        "    'Category B': [20, 15, 25]\n",
        "}, index=['Group 1', 'Group 2', 'Group 3'])\n",
        "\n",
        "print(\"Observed Frequencies:\")\n",
        "print(observed_data)\n",
        "\n",
        "expected_freq = calculate_expected_frequencies(observed_data)\n",
        "\n",
        "print(\"\\nExpected Frequencies:\")\n",
        "print(expected_freq)\n"
      ],
      "metadata": {
        "id": "AwHppYKc8dI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques14) Perform a goodness-of-fit test using Python to compare the observed data to an expected distribution?"
      ],
      "metadata": {
        "id": "7_Rxeo4f8hmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import chisquare\n",
        "\n",
        "# Example observed frequencies (e.g., counts of categories)\n",
        "observed = [50, 30, 20]\n",
        "\n",
        "# Example expected distribution (as probabilities or counts)\n",
        "# If probabilities, multiply by total count to get expected counts\n",
        "expected_proportions = [0.4, 0.4, 0.2]\n",
        "total_count = sum(observed)\n",
        "expected = [p * total_count for p in expected_proportions]\n",
        "\n",
        "print(\"Observed frequencies:\", observed)\n",
        "print(\"Expected frequencies:\", expected)\n",
        "\n",
        "# Perform the Chi-square goodness-of-fit test\n",
        "chi2_stat, p_value = chisquare(f_obs=observed, f_exp=expected)\n",
        "\n",
        "print(f\"\\nChi-square Statistic = {chi2_stat:.4f}\")\n",
        "print(f\"P-value = {p_value:.4f}\")\n",
        "\n",
        "# Interpretation\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis: observed data does NOT fit the expected distribution.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: observed data fits the expected distribution.\")\n"
      ],
      "metadata": {
        "id": "6tXHXdJt8kvY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}